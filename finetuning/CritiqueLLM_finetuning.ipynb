{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and client initialisation"
      ],
      "metadata": {
        "id": "-ZM3lCWLaFXW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CsKt8N02fCk",
        "outputId": "980c0792-5d1d-4e39-bc61-25ce3c6c04a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q mistralai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "import os\n",
        "\n",
        "api_key = \"\"\n",
        "\n",
        "client = MistralClient(api_key=api_key)"
      ],
      "metadata": {
        "id": "09nKMvG72k4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning mistral-small-latest on 50 data points"
      ],
      "metadata": {
        "id": "OhRKe8UI28q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the data format"
      ],
      "metadata": {
        "id": "QOL1vloJ6MHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the validation and reformat script\n",
        "!wget https://raw.githubusercontent.com/mistralai/mistral-finetune/main/utils/reformat_data.py\n",
        "\n",
        "# validate and reform the training data\n",
        "!python reformat_data.py /content/drive/MyDrive/train_data.jsonl\n",
        "\n",
        "# validate and reform the validation data\n",
        "!python reformat_data.py /content/drive/MyDrive/validation_data.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXy4Orhd6arS",
        "outputId": "fa0e896f-1b9b-403b-d9b1-625000c40649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-28 17:35:30--  https://raw.githubusercontent.com/mistralai/mistral-finetune/main/utils/reformat_data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3381 (3.3K) [text/plain]\n",
            "Saving to: ‘reformat_data.py.2’\n",
            "\n",
            "\rreformat_data.py.2    0%[                    ]       0  --.-KB/s               \rreformat_data.py.2  100%[===================>]   3.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-28 17:35:30 (44.2 MB/s) - ‘reformat_data.py.2’ saved [3381/3381]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the files in client"
      ],
      "metadata": {
        "id": "ePgaIj-578wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/train_data.jsonl\", \"rb\") as f:\n",
        "    ultrachat_chunk_train = client.files.create(file=(\"train_data.jsonl\", f))\n",
        "with open(\"/content/drive/MyDrive/validation_data.jsonl\", \"rb\") as f:\n",
        "    ultrachat_chunk_eval = client.files.create(file=(\"validation_data.jsonl\", f))"
      ],
      "metadata": {
        "id": "5LZur-8y7uE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimating number of train steps"
      ],
      "metadata": {
        "id": "h8vDoxFT6gJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "approximate_epochs = 100\n",
        "\n",
        "def get_size_in_mb(file_path: str) -> float:\n",
        "    file_size_bytes = os.path.getsize(file_path)\n",
        "    file_size_mb = file_size_bytes / (1000 * 1000)\n",
        "    return file_size_mb\n",
        "\n",
        "size_file = get_size_in_mb(\"/content/drive/MyDrive/train_data.jsonl\")\n",
        "print(\"File Size:\", size_file, \"mb\")\n",
        "training_steps = int(approximate_epochs * size_file)\n",
        "print(\"Training steps:\", training_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrdHMonM6z-m",
        "outputId": "459ef700-a67f-467d-9262-8ef54a55f839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size: 0.269834 mb\n",
            "Training steps: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a fine-tuning job and visualising in WB"
      ],
      "metadata": {
        "id": "j7M7BhNo63X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.models.jobs import WandbIntegrationIn, TrainingParameters\n",
        "\n",
        "wandb_api_key = \"\" # Removed the api key\n",
        "\n",
        "created_jobs = client.jobs.create(\n",
        "    model=\"mistral-small-latest\",\n",
        "    training_files=[ultrachat_chunk_train.id],\n",
        "    validation_files=[ultrachat_chunk_eval.id],\n",
        "    hyperparameters=TrainingParameters(\n",
        "        training_steps=26,\n",
        "        learning_rate=0.0001,\n",
        "    ),\n",
        "    integrations=[\n",
        "        WandbIntegrationIn(\n",
        "            project=\"test_api\",\n",
        "            run_name=\"test\",\n",
        "            api_key=wandb_api_key,\n",
        "        ).dict()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "VLMhlzhO7kSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(created_jobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laFcyuLP840_",
        "outputId": "ab889762-f5df-4474-a6fc-6b5fda4c72e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id='9672d988-d6c9-4007-816f-ce6895c94ba6' hyperparameters=TrainingParameters(training_steps=26, learning_rate=0.0001) fine_tuned_model=None model='mistral-small-latest' status='QUEUED' job_type='FT' created_at=1719596804 modified_at=1719596804 training_files=['a0f75376-861c-4c96-bf8a-14a58f246221'] validation_files=['527c7872-a89d-4e40-9dc1-b7ca2123fc0f'] object='job' integrations=[WandbIntegration(type='wandb', project='test_api', name=None, run_name='test')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve the finetuned mistral-small-latest model"
      ],
      "metadata": {
        "id": "1dCN0c07_6sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a jobs\n",
        "retrieved_jobs = client.jobs.retrieve(created_jobs.id)\n",
        "print(retrieved_jobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX0zrYHe9QKV",
        "outputId": "9ec08eca-a70d-4893-f3d0-df4edf5a55a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id='9672d988-d6c9-4007-816f-ce6895c94ba6' hyperparameters=TrainingParameters(training_steps=26, learning_rate=0.0001) fine_tuned_model='ft:mistral-small-latest:38a3e9c5:20240628:9672d988' model='mistral-small-latest' status='SUCCESS' job_type='FT' created_at=1719596804 modified_at=1719597269 training_files=['a0f75376-861c-4c96-bf8a-14a58f246221'] validation_files=['527c7872-a89d-4e40-9dc1-b7ca2123fc0f'] object='job' integrations=[WandbIntegration(type='wandb', project='test_api', name=None, run_name='test')] events=[Event(name='status-updated', data={'status': 'SUCCESS'}, created_at=1719597269), Event(name='status-updated', data={'status': 'RUNNING'}, created_at=1719596805), Event(name='status-updated', data={'status': 'QUEUED'}, created_at=1719596804)] checkpoints=[Checkpoint(metrics=Metric(train_loss=0.063598, valid_loss=0.553382, valid_mean_token_accuracy=1.467522), step_number=20, created_at=1719597134), Checkpoint(metrics=Metric(train_loss=0.353003, valid_loss=0.478452, valid_mean_token_accuracy=1.393248), step_number=10, created_at=1719596986)] estimated_start_time=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning open-mistral-7b on 50 data points\n",
        "\n"
      ],
      "metadata": {
        "id": "U9sYnMevCFL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.models.jobs import WandbIntegrationIn, TrainingParameters\n",
        "\n",
        "wandb_api_key = \"\" # Removed the api key\n",
        "\n",
        "created_jobs = client.jobs.create(\n",
        "    model=\"open-mistral-7b\",\n",
        "    training_files=[ultrachat_chunk_train.id],\n",
        "    validation_files=[ultrachat_chunk_eval.id],\n",
        "    hyperparameters=TrainingParameters(\n",
        "        training_steps=30,\n",
        "        learning_rate=0.0001,\n",
        "    ),\n",
        "    integrations=[\n",
        "        WandbIntegrationIn(\n",
        "            project=\"test_api\",\n",
        "            run_name=\"test\",\n",
        "            api_key=wandb_api_key,\n",
        "        ).dict()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "rTDqhWbzjNHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(created_jobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ0RttVojjlO",
        "outputId": "417c139a-3d73-419a-ea1d-2479fb25b535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id='a0588788-2d7f-4aa8-acb0-5f46e29ff13a' hyperparameters=TrainingParameters(training_steps=30, learning_rate=0.0001) fine_tuned_model=None model='open-mistral-7b' status='QUEUED' job_type='FT' created_at=1719606853 modified_at=1719606853 training_files=['a0f75376-861c-4c96-bf8a-14a58f246221'] validation_files=['527c7872-a89d-4e40-9dc1-b7ca2123fc0f'] object='job' integrations=[WandbIntegration(type='wandb', project='test_api', name=None, run_name='test')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve the finetuned open-mistral-7b model"
      ],
      "metadata": {
        "id": "_4BnynaYa3uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a jobs\n",
        "retrieved_jobs = client.jobs.retrieve(created_jobs.id)\n",
        "print(retrieved_jobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS7AUvIEjlGs",
        "outputId": "9c3dc45f-b861-4da7-edd5-4d6a3f4e25bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id='a0588788-2d7f-4aa8-acb0-5f46e29ff13a' hyperparameters=TrainingParameters(training_steps=30, learning_rate=0.0001) fine_tuned_model='ft:open-mistral-7b:38a3e9c5:20240628:a0588788' model='open-mistral-7b' status='SUCCESS' job_type='FT' created_at=1719606853 modified_at=1719607068 training_files=['a0f75376-861c-4c96-bf8a-14a58f246221'] validation_files=['527c7872-a89d-4e40-9dc1-b7ca2123fc0f'] object='job' integrations=[WandbIntegration(type='wandb', project='test_api', name=None, run_name='test')] events=[Event(name='status-updated', data={'status': 'SUCCESS'}, created_at=1719607068), Event(name='status-updated', data={'status': 'RUNNING'}, created_at=1719606853), Event(name='status-updated', data={'status': 'QUEUED'}, created_at=1719606853)] checkpoints=[Checkpoint(metrics=Metric(train_loss=0.171175, valid_loss=0.663745, valid_mean_token_accuracy=1.58419), step_number=30, created_at=1719607035), Checkpoint(metrics=Metric(train_loss=0.250904, valid_loss=0.620961, valid_mean_token_accuracy=1.5379), step_number=20, created_at=1719606985), Checkpoint(metrics=Metric(train_loss=0.530758, valid_loss=0.580973, valid_mean_token_accuracy=1.495857), step_number=10, created_at=1719606925)] estimated_start_time=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally we have decided on using open-mistral-7b finetuned due to less latency and comparable output quality to mistral-small-latest finetuned"
      ],
      "metadata": {
        "id": "-TJYcoTmbP3b"
      }
    }
  ]
}